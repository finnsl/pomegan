{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stylegan2-pomological-generation.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stylegan2 to generate watercolor fruit\n",
        "\n",
        "This notebook is the outline of how to modify minimally stylegan2 (taken from [the nvidia github repository](https://github.com/NVlabs/stylegan2-ada-pytorch) ) to work on the [USDA Pomological watercolors](https://naldc.nal.usda.gov/usda_pomological_watercolor) the middle resolution thumbnails of which were collected and stored as part of the [github repository](https://github.com/jwilber/USDA_Pomological_Watercolors) of jwilber. \n",
        "\n",
        "### Current flow\n",
        "1. Import the model and the data from the respective github repositories and mount google drive to store the data output files in a way that means you only have to do this step once.\n",
        "2. The model is modified slightly to work with pytorch versions above 1.9 - to do this one must make sure ninja is installed and also to modify two of the custom scripts to contain references to the later versions - the file names are specifiied before the model training cells.\n",
        "\n",
        "### TODO\n",
        "Store the weights on github and clone them for transfer learning further fruit.\n",
        "\n",
        "Add examples of the fruit generated.\n",
        "\n",
        "### Remarks about output\n",
        "\n",
        "Examples of the output can be found in the github repositiory (not yet). The output has a FID score of 58 after only 700 steps - which is much less than the recommended 5000 steps for minimum quality output suggested by the original [paper](https://arxiv.org/pdf/2006.06676.pdf). "
      ],
      "metadata": {
        "id": "2i10T1DfO9Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the model and data, mounting drive"
      ],
      "metadata": {
        "id": "8JabxVdpQ0WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJfFs_wsdiM2",
        "outputId": "9d9d3e7a-f4dd-4123-a349-cef80acbbdd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNXsW1_wcAYb",
        "outputId": "9625cd28-7015-4a12-8dcd-947c3fb4e846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 2.20 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Cloning into 'USDA_Pomological_Watercolors'...\n",
            "remote: Enumerating objects: 7615, done.\u001b[K\n",
            "remote: Total 7615 (delta 0), reused 0 (delta 0), pack-reused 7615\u001b[K\n",
            "Receiving objects: 100% (7615/7615), 33.25 MiB | 11.33 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch\n",
        "!git clone https://github.com/jwilber/USDA_Pomological_Watercolors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manipulating the images using a commandline tool to put them into the format that the model can process."
      ],
      "metadata": {
        "id": "80Qxqc28Q8Mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model can take only square images that are width/height a power of 2 - I previously used them as 256x256 images, but the training time would be lower at 128x128 - below I installed imagemagick's unix standalone command line tool, made it executable and then did the conversion in size and to pngs (from jpgs) that the model requires. This takes about 10 minutes."
      ],
      "metadata": {
        "id": "d7A4xQTORA5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/USDA_Pomological_Watercolors/data/images/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lKbLjw9dimX",
        "outputId": "c2062507-8286-4ea5-aca5-b09116225989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/USDA_Pomological_Watercolors/data/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pngs"
      ],
      "metadata": {
        "id": "qlnkG1Y0gQLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.imagemagick.org/ImageMagick/download/binaries/magick"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDfqllLsgfPP",
        "outputId": "4cddf667-1d60-4a53-f1e4-730faea16d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-31 13:47:28--  https://download.imagemagick.org/ImageMagick/download/binaries/magick\n",
            "Resolving download.imagemagick.org (download.imagemagick.org)... 50.251.58.13\n",
            "Connecting to download.imagemagick.org (download.imagemagick.org)|50.251.58.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27104448 (26M)\n",
            "Saving to: ‘magick’\n",
            "\n",
            "magick              100%[===================>]  25.85M   573KB/s    in 40s     \n",
            "\n",
            "2022-03-31 13:48:09 (663 KB/s) - ‘magick’ saved [27104448/27104448]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x magick"
      ],
      "metadata": {
        "id": "YQwPiAcyjTTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./magick '*.jpg[256x256!]' image%04d.png"
      ],
      "metadata": {
        "id": "BqYMRaj1dgCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv *.png /content/USDA_Pomological_Watercolors/data/images/pngs/"
      ],
      "metadata": {
        "id": "jyBlSEollqFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here you can adjust the paths to your own specficiations - note down where you are working you will need it later.\n",
        "!mkdir /content/drive/MyDrive/datascience/poms/pngs"
      ],
      "metadata": {
        "id": "F3YwQmnHl_0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/USDA_Pomological_Watercolors/data/images/pngs/*.png /content/drive/MyDrive/datascience/poms/pngs/ "
      ],
      "metadata": {
        "id": "CjQubv16mI2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring the dataset and the model"
      ],
      "metadata": {
        "id": "y7f1XoMeRby5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run our newly minted pngs through their data preparation script to produce a viable zip file of pngs, in our case, in this instance, without labels.\n",
        "\n",
        "Then we are ready to run the model - with small numbers of steps between savings and for a short-ish length. This will be a fresh training, with the step below to execute for a continuation/resumation of the training at a later time.\n",
        "\n",
        "This requires us to manipulate two of the configuration files to enable later versions of pytorch beyond what they're expecting, and to install packages to allow them to perform image manipulations every iteration of their model."
      ],
      "metadata": {
        "id": "arViHs7ORgqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/stylegan2-ada-pytorch/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zJ2gZUJnSsM",
        "outputId": "cbd67ae1-c597-40ce-d949-0aaceb5591a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-ada-pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python dataset_tool.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msvil2bYoqpB",
        "outputId": "1743ce88-771f-4a62-886c-ca6b169530be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: dataset_tool.py [OPTIONS]\n",
            "\n",
            "  Convert an image dataset into a dataset\n",
            "  archive usable with StyleGAN2 ADA PyTorch.\n",
            "\n",
            "  The input dataset format is guessed from the\n",
            "  --source argument:\n",
            "\n",
            "  --source *_lmdb/                    Load LSUN dataset\n",
            "  --source cifar-10-python.tar.gz     Load CIFAR-10 dataset\n",
            "  --source train-images-idx3-ubyte.gz Load MNIST dataset\n",
            "  --source path/                      Recursively load all images from path/\n",
            "  --source dataset.zip                Recursively load all images from dataset.zip\n",
            "\n",
            "  Specifying the output format and path:\n",
            "\n",
            "  --dest /path/to/dir                 Save output files under /path/to/dir\n",
            "  --dest /path/to/dataset.zip         Save output files into /path/to/dataset.zip\n",
            "\n",
            "  The output dataset format can be either an\n",
            "  image folder or an uncompressed zip archive.\n",
            "  Zip archives makes it easier to move datasets\n",
            "  around file servers and clusters, and may\n",
            "  offer better training performance on network\n",
            "  file systems.\n",
            "\n",
            "  Images within the dataset archive will be\n",
            "  stored as uncompressed PNG. Uncompresed PNGs\n",
            "  can be efficiently decoded in the training\n",
            "  loop.\n",
            "\n",
            "  Class labels are stored in a file called\n",
            "  'dataset.json' that is stored at the dataset\n",
            "  root folder.  This file has the following\n",
            "  structure:\n",
            "\n",
            "  {\n",
            "      \"labels\": [\n",
            "          [\"00000/img00000000.png\",6],\n",
            "          [\"00000/img00000001.png\",9],\n",
            "          ... repeated for every image in the datase\n",
            "          [\"00049/img00049999.png\",1]\n",
            "      ]\n",
            "  }\n",
            "\n",
            "  If the 'dataset.json' file cannot be found,\n",
            "  the dataset is interpreted as not containing\n",
            "  class labels.\n",
            "\n",
            "  Image scale/crop and resolution requirements:\n",
            "\n",
            "  Output images must be square-shaped and they\n",
            "  must all have the same power-of-two\n",
            "  dimensions.\n",
            "\n",
            "  To scale arbitrary input image size to a\n",
            "  specific width and height, use the --width and\n",
            "  --height options.  Output resolution will be\n",
            "  either the original input resolution (if\n",
            "  --width/--height was not specified) or the one\n",
            "  specified with --width/height.\n",
            "\n",
            "  Use the --transform=center-crop or\n",
            "  --transform=center-crop-wide options to apply\n",
            "  a center crop transform on the input image.\n",
            "  These options should be used with the --width\n",
            "  and --height options.  For example:\n",
            "\n",
            "  python dataset_tool.py --source LSUN/raw/cat_lmdb --dest /tmp/lsun_cat \\\n",
            "      --transform=center-crop-wide --width 512 --height=384\n",
            "\n",
            "Options:\n",
            "  --source PATH                   Directory or\n",
            "                                  archive name for\n",
            "                                  input dataset\n",
            "                                  [required]\n",
            "\n",
            "  --dest PATH                     Output directory\n",
            "                                  or archive name\n",
            "                                  for output\n",
            "                                  dataset\n",
            "                                  [required]\n",
            "\n",
            "  --max-images INTEGER            Output only up\n",
            "                                  to `max-images`\n",
            "                                  images\n",
            "\n",
            "  --resize-filter [box|lanczos]   Filter to use\n",
            "                                  when resizing\n",
            "                                  images for\n",
            "                                  output\n",
            "                                  resolution\n",
            "                                  [default:\n",
            "                                  lanczos]\n",
            "\n",
            "  --transform [center-crop|center-crop-wide]\n",
            "                                  Input\n",
            "                                  crop/resize mode\n",
            "\n",
            "  --width INTEGER                 Output width\n",
            "  --height INTEGER                Output height\n",
            "  --help                          Show this\n",
            "                                  message and\n",
            "                                  exit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You must adjust the path to the pngs you created previously.\n",
        "!python dataset_tool.py --source /content/drive/MyDrive/datascience/poms/pngs/ --dest /content/drive/MyDrive/datascience/poms/data_pngs.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJeK2bSinkzP",
        "outputId": "022a6bf4-51e7-4d9b-bccc-dfaf1780967b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 7580/7580 [01:46<00:00, 71.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzLfILUmp9ds",
        "outputId": "aee08c67-dc75-451e-e0dd-6a70cd7d7cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "Usage: train.py [OPTIONS]\n",
            "\n",
            "  Train a GAN using the techniques described in\n",
            "  the paper \"Training Generative Adversarial\n",
            "  Networks with Limited Data\".\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  # Train with custom dataset using 1 GPU.\n",
            "  python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1\n",
            "\n",
            "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
            "  python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\\n",
            "      --gpus=2 --cfg=cifar --cond=1\n",
            "\n",
            "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
            "  python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\\n",
            "      --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
            "\n",
            "  # Reproduce original StyleGAN2 config F.\n",
            "  python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\\n",
            "      --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug\n",
            "\n",
            "  Base configs (--cfg):\n",
            "    auto       Automatically select reasonable defaults based on resolution\n",
            "               and GPU count. Good starting point for new datasets.\n",
            "    stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.\n",
            "    paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
            "    paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
            "    paper1024  Reproduce results for MetFaces at 1024x1024.\n",
            "    cifar      Reproduce results for CIFAR-10 at 32x32.\n",
            "\n",
            "  Transfer learning source networks (--resume):\n",
            "    ffhq256        FFHQ trained at 256x256 resolution.\n",
            "    ffhq512        FFHQ trained at 512x512 resolution.\n",
            "    ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
            "    celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
            "    lsundog256     LSUN Dog trained at 256x256 resolution.\n",
            "    <PATH or URL>  Custom network pickle.\n",
            "\n",
            "Options:\n",
            "  --outdir DIR                    Where to save\n",
            "                                  the results\n",
            "                                  [required]\n",
            "\n",
            "  --gpus INT                      Number of GPUs\n",
            "                                  to use [default:\n",
            "                                  1]\n",
            "\n",
            "  --snap INT                      Snapshot\n",
            "                                  interval\n",
            "                                  [default: 50\n",
            "                                  ticks]\n",
            "\n",
            "  --metrics LIST                  Comma-separated\n",
            "                                  list or \"none\"\n",
            "                                  [default:\n",
            "                                  fid50k_full]\n",
            "\n",
            "  --seed INT                      Random seed\n",
            "                                  [default: 0]\n",
            "\n",
            "  -n, --dry-run                   Print training\n",
            "                                  options and exit\n",
            "\n",
            "  --data PATH                     Training data\n",
            "                                  (directory or\n",
            "                                  zip)  [required]\n",
            "\n",
            "  --cond BOOL                     Train\n",
            "                                  conditional\n",
            "                                  model based on\n",
            "                                  dataset labels\n",
            "                                  [default: false]\n",
            "\n",
            "  --subset INT                    Train with only\n",
            "                                  N images\n",
            "                                  [default: all]\n",
            "\n",
            "  --mirror BOOL                   Enable dataset\n",
            "                                  x-flips\n",
            "                                  [default: false]\n",
            "\n",
            "  --cfg [auto|stylegan2|paper256|paper512|paper1024|cifar]\n",
            "                                  Base config\n",
            "                                  [default: auto]\n",
            "\n",
            "  --gamma FLOAT                   Override R1\n",
            "                                  gamma\n",
            "\n",
            "  --kimg INT                      Override\n",
            "                                  training\n",
            "                                  duration\n",
            "\n",
            "  --batch INT                     Override batch\n",
            "                                  size\n",
            "\n",
            "  --aug [noaug|ada|fixed]         Augmentation\n",
            "                                  mode [default:\n",
            "                                  ada]\n",
            "\n",
            "  --p FLOAT                       Augmentation\n",
            "                                  probability for\n",
            "                                  --aug=fixed\n",
            "\n",
            "  --target FLOAT                  ADA target value\n",
            "                                  for --aug=ada\n",
            "\n",
            "  --augpipe [blit|geom|color|filter|noise|cutout|bg|bgc|bgcf|bgcfn|bgcfnc]\n",
            "                                  Augmentation\n",
            "                                  pipeline\n",
            "                                  [default: bgc]\n",
            "\n",
            "  --resume PKL                    Resume training\n",
            "                                  [default:\n",
            "                                  noresume]\n",
            "\n",
            "  --freezed INT                   Freeze-D\n",
            "                                  [default: 0\n",
            "                                  layers]\n",
            "\n",
            "  --fp32 BOOL                     Disable mixed-\n",
            "                                  precision\n",
            "                                  training\n",
            "\n",
            "  --nhwc BOOL                     Use NHWC memory\n",
            "                                  format with FP16\n",
            "\n",
            "  --nobench BOOL                  Disable cuDNN\n",
            "                                  benchmarking\n",
            "\n",
            "  --allow-tf32 BOOL               Allow PyTorch to\n",
            "                                  use TF32\n",
            "                                  internally\n",
            "\n",
            "  --workers INT                   Override number\n",
            "                                  of DataLoader\n",
            "                                  workers\n",
            "\n",
            "  --help                          Show this\n",
            "                                  message and\n",
            "                                  exit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#again, make a new directory to store the output - this is mine.\n",
        "!mkdir /content/drive/MyDrive/datascience/poms/sgtrain/"
      ],
      "metadata": {
        "id": "xYNd32ngqQFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One must edit the following files: \n",
        "\n",
        "\n",
        "*   /content/stylegan2-ada-pytorch/torch_utils/ops/conv2d_gradfix.py\n",
        "*   /content/stylegan2-ada-pytorch/torch_utils/ops/grid_sample_gradfix.py\n",
        "\n",
        "to include '1.10' in lists of versions available in the version checking steps - which can be found on line 53 for conv2d_gradfix.py and line 37 for grid_sample_gradfix.py."
      ],
      "metadata": {
        "id": "p_J2bw_7UrYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# these are the packages required - ninja is particularly important for their custom image manipulation libaries. \n",
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Da81xiogsDJM",
        "outputId": "ae42fc49-0cb7-4f74-fa86-0a5da7cf08d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n",
            "Collecting pyspng\n",
            "  Downloading pyspng-0.1.0-cp37-cp37m-manylinux2010_x86_64.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 31.3 MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 79.5 MB/s \n",
            "\u001b[?25hCollecting imageio-ffmpeg==0.4.3\n",
            "  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyspng) (1.21.5)\n",
            "Installing collected packages: pyspng, ninja, imageio-ffmpeg\n",
            "Successfully installed imageio-ffmpeg-0.4.3 ninja-1.10.2.3 pyspng-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To train fresh, remove the --resume path. On the high ram/gpu colab pro machine (p100 gpu) this particular training routine took ~5m/tick, ~15m for a scoring tick and the entire thing lasted 6h49m.\n",
        "!python train.py --resume /content/drive/MyDrive/datascience/poms/sgtrain/00003-data_pngs-auto1-kimg1000/network-snapshot-000400.pkl --outdir /content/drive/MyDrive/datascience/poms/sgtrain/ --data /content/drive/MyDrive/datascience/poms/data_pngs.zip --gpus 1 --kimg 300 --snap 15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKRGGm2Bqhni",
        "outputId": "4c754ea8-d2fa-4096-e09b-e34163816808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 15,\n",
            "  \"network_snapshot_ticks\": 15,\n",
            "  \"metrics\": [\n",
            "    \"fid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/datascience/poms/data_pngs.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 7580,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 0.8192\n",
            "  },\n",
            "  \"total_kimg\": 300,\n",
            "  \"batch_size\": 16,\n",
            "  \"batch_gpu\": 16,\n",
            "  \"ema_kimg\": 5.0,\n",
            "  \"ema_rampup\": null,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"/content/drive/MyDrive/datascience/poms/sgtrain/00003-data_pngs-auto1-kimg1000/network-snapshot-000400.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"/content/drive/MyDrive/datascience/poms/sgtrain/00006-data_pngs-auto1-kimg300-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/datascience/poms/sgtrain/00006-data_pngs-auto1-kimg300-resumecustom\n",
            "Training data:      /content/drive/MyDrive/datascience/poms/data_pngs.zip\n",
            "Training duration:  300 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   7580\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  7580\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"/content/drive/MyDrive/datascience/poms/sgtrain/00003-data_pngs-auto1-kimg1000/network-snapshot-000400.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape         Datatype\n",
            "---                   ---         ---      ---                  ---     \n",
            "mapping.fc0           262656      -        [16, 512]            float32 \n",
            "mapping.fc1           262656      -        [16, 512]            float32 \n",
            "mapping               -           512      [16, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [16, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [16, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [16, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [16, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [16, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [16, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [16, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [16, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [16, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [16, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [16, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [16, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [16, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [16, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [16, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [16, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [16, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [16, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   132099      -        [16, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [16, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [16, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [16, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [16, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  66051       -        [16, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [16, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [16, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [16, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [16, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [16, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [16, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [16, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                  ---     \n",
            "Total                 23191522    175568   -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b256.fromrgb   256         16       [16, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [16, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [16, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [16, 128, 128, 128]  float16 \n",
            "b256           -           16       [16, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [16, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [16, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [16, 256, 64, 64]    float16 \n",
            "b128           -           16       [16, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [16, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [16, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [16, 512, 32, 32]    float16 \n",
            "b64            -           16       [16, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [16, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [16, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [16, 512, 16, 16]    float16 \n",
            "b32            -           16       [16, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [16, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [16, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [16, 512, 8, 8]      float32 \n",
            "b16            -           16       [16, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [16, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [16, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [16, 512, 4, 4]      float32 \n",
            "b8             -           16       [16, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [16, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [16, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [16, 512]            float32 \n",
            "b4.out         513         -        [16, 1]              float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          24001089    416      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 300 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 1m 22s       sec/tick 8.5     sec/kimg 528.86  maintenance 73.2   cpumem 7.10   gpumem 10.38  augment 0.000\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 99.56150100519241}, \"metric\": \"fid50k_full\", \"total_time\": 631.4616429805756, \"total_time_str\": \"10m 31s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1648458071.9642816}\n",
            "tick 1     kimg 4.0      time 16m 40s      sec/tick 268.7   sec/kimg 67.16   maintenance 649.2  cpumem 7.75   gpumem 5.35   augment 0.023\n",
            "tick 2     kimg 8.0      time 21m 11s      sec/tick 270.9   sec/kimg 67.72   maintenance 0.1    cpumem 7.75   gpumem 4.84   augment 0.051\n",
            "tick 3     kimg 12.0     time 25m 41s      sec/tick 270.0   sec/kimg 67.49   maintenance 0.1    cpumem 7.75   gpumem 4.81   augment 0.061\n",
            "tick 4     kimg 16.0     time 30m 13s      sec/tick 271.7   sec/kimg 67.92   maintenance 0.1    cpumem 7.75   gpumem 4.83   augment 0.076\n",
            "tick 5     kimg 20.0     time 34m 43s      sec/tick 270.3   sec/kimg 67.59   maintenance 0.1    cpumem 7.75   gpumem 4.85   augment 0.083\n",
            "tick 6     kimg 24.0     time 39m 13s      sec/tick 270.3   sec/kimg 67.58   maintenance 0.1    cpumem 7.75   gpumem 4.87   augment 0.089\n",
            "tick 7     kimg 28.0     time 43m 45s      sec/tick 271.0   sec/kimg 67.75   maintenance 0.1    cpumem 7.75   gpumem 4.83   augment 0.081\n",
            "tick 8     kimg 32.0     time 48m 16s      sec/tick 271.3   sec/kimg 67.82   maintenance 0.1    cpumem 7.75   gpumem 4.84   augment 0.076\n",
            "tick 9     kimg 36.0     time 52m 49s      sec/tick 272.5   sec/kimg 68.13   maintenance 0.2    cpumem 7.75   gpumem 4.84   augment 0.074\n",
            "tick 10    kimg 40.0     time 57m 26s      sec/tick 277.3   sec/kimg 69.32   maintenance 0.2    cpumem 7.75   gpumem 4.87   augment 0.074\n",
            "tick 11    kimg 44.0     time 1h 02m 02s   sec/tick 276.1   sec/kimg 69.03   maintenance 0.1    cpumem 7.75   gpumem 4.83   augment 0.081\n",
            "tick 12    kimg 48.0     time 1h 06m 39s   sec/tick 276.9   sec/kimg 69.23   maintenance 0.2    cpumem 7.75   gpumem 4.82   augment 0.090\n",
            "tick 13    kimg 52.0     time 1h 11m 16s   sec/tick 276.1   sec/kimg 69.03   maintenance 0.1    cpumem 7.75   gpumem 4.84   augment 0.092\n",
            "tick 14    kimg 56.0     time 1h 15m 52s   sec/tick 276.3   sec/kimg 69.08   maintenance 0.2    cpumem 7.75   gpumem 4.86   augment 0.083\n",
            "tick 15    kimg 60.0     time 1h 20m 29s   sec/tick 276.9   sec/kimg 69.23   maintenance 0.1    cpumem 7.75   gpumem 4.84   augment 0.088\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 96.35993348463943}, \"metric\": \"fid50k_full\", \"total_time\": 601.5340616703033, \"total_time_str\": \"10m 02s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000060.pkl\", \"timestamp\": 1648462790.6895351}\n",
            "tick 16    kimg 64.0     time 1h 35m 27s   sec/tick 277.8   sec/kimg 69.46   maintenance 620.3  cpumem 8.10   gpumem 4.86   augment 0.090\n",
            "tick 17    kimg 68.0     time 1h 40m 05s   sec/tick 277.1   sec/kimg 69.26   maintenance 0.2    cpumem 8.10   gpumem 4.86   augment 0.090\n",
            "tick 18    kimg 72.0     time 1h 44m 43s   sec/tick 277.7   sec/kimg 69.43   maintenance 0.2    cpumem 8.10   gpumem 4.85   augment 0.080\n",
            "tick 19    kimg 76.0     time 1h 49m 20s   sec/tick 276.8   sec/kimg 69.20   maintenance 0.2    cpumem 8.10   gpumem 4.82   augment 0.080\n",
            "tick 20    kimg 80.0     time 1h 53m 57s   sec/tick 277.5   sec/kimg 69.37   maintenance 0.1    cpumem 8.10   gpumem 4.87   augment 0.083\n",
            "tick 21    kimg 84.0     time 1h 58m 34s   sec/tick 277.1   sec/kimg 69.29   maintenance 0.2    cpumem 8.10   gpumem 4.87   augment 0.080\n",
            "tick 22    kimg 88.0     time 2h 03m 12s   sec/tick 277.5   sec/kimg 69.37   maintenance 0.2    cpumem 8.10   gpumem 4.84   augment 0.084\n",
            "tick 23    kimg 92.0     time 2h 07m 50s   sec/tick 277.4   sec/kimg 69.34   maintenance 0.2    cpumem 8.10   gpumem 4.85   augment 0.084\n",
            "tick 24    kimg 96.0     time 2h 12m 27s   sec/tick 277.5   sec/kimg 69.38   maintenance 0.1    cpumem 8.10   gpumem 4.87   augment 0.100\n",
            "tick 25    kimg 100.0    time 2h 17m 04s   sec/tick 276.8   sec/kimg 69.19   maintenance 0.2    cpumem 8.10   gpumem 4.87   augment 0.099\n",
            "tick 26    kimg 104.0    time 2h 21m 42s   sec/tick 277.3   sec/kimg 69.32   maintenance 0.2    cpumem 8.10   gpumem 4.90   augment 0.098\n",
            "tick 27    kimg 108.0    time 2h 26m 19s   sec/tick 276.8   sec/kimg 69.20   maintenance 0.2    cpumem 8.10   gpumem 4.85   augment 0.088\n",
            "tick 28    kimg 112.0    time 2h 30m 56s   sec/tick 277.1   sec/kimg 69.28   maintenance 0.2    cpumem 8.10   gpumem 4.83   augment 0.091\n",
            "tick 29    kimg 116.0    time 2h 35m 33s   sec/tick 277.0   sec/kimg 69.25   maintenance 0.2    cpumem 8.10   gpumem 4.83   augment 0.092\n",
            "tick 30    kimg 120.0    time 2h 40m 10s   sec/tick 277.4   sec/kimg 69.34   maintenance 0.1    cpumem 8.10   gpumem 4.85   augment 0.095\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 77.22994998780126}, \"metric\": \"fid50k_full\", \"total_time\": 598.4336264133453, \"total_time_str\": \"9m 58s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000120.pkl\", \"timestamp\": 1648467568.3674295}\n",
            "tick 31    kimg 124.0    time 2h 55m 04s   sec/tick 277.0   sec/kimg 69.25   maintenance 616.8  cpumem 8.10   gpumem 4.85   augment 0.092\n",
            "tick 32    kimg 128.0    time 2h 59m 42s   sec/tick 277.8   sec/kimg 69.44   maintenance 0.1    cpumem 8.10   gpumem 4.85   augment 0.095\n",
            "tick 33    kimg 132.0    time 3h 04m 19s   sec/tick 276.4   sec/kimg 69.09   maintenance 0.2    cpumem 8.10   gpumem 4.85   augment 0.087\n",
            "tick 34    kimg 136.0    time 3h 08m 56s   sec/tick 277.5   sec/kimg 69.38   maintenance 0.1    cpumem 8.10   gpumem 4.83   augment 0.084\n",
            "tick 35    kimg 140.0    time 3h 13m 33s   sec/tick 276.7   sec/kimg 69.18   maintenance 0.2    cpumem 8.10   gpumem 4.87   augment 0.085\n",
            "tick 36    kimg 144.0    time 3h 18m 11s   sec/tick 277.6   sec/kimg 69.41   maintenance 0.2    cpumem 8.10   gpumem 4.86   augment 0.077\n",
            "tick 37    kimg 148.0    time 3h 22m 49s   sec/tick 277.4   sec/kimg 69.35   maintenance 0.2    cpumem 8.10   gpumem 4.84   augment 0.085\n",
            "tick 38    kimg 152.0    time 3h 27m 25s   sec/tick 276.1   sec/kimg 69.03   maintenance 0.2    cpumem 8.10   gpumem 4.87   augment 0.077\n",
            "tick 39    kimg 156.0    time 3h 32m 02s   sec/tick 276.5   sec/kimg 69.14   maintenance 0.2    cpumem 8.10   gpumem 4.82   augment 0.074\n",
            "tick 40    kimg 160.0    time 3h 36m 39s   sec/tick 277.2   sec/kimg 69.29   maintenance 0.2    cpumem 8.10   gpumem 4.87   augment 0.075\n",
            "tick 41    kimg 164.0    time 3h 41m 15s   sec/tick 276.1   sec/kimg 69.03   maintenance 0.2    cpumem 8.10   gpumem 4.89   augment 0.075\n",
            "tick 42    kimg 168.0    time 3h 45m 53s   sec/tick 277.5   sec/kimg 69.37   maintenance 0.2    cpumem 8.10   gpumem 4.90   augment 0.081\n",
            "tick 43    kimg 172.0    time 3h 50m 29s   sec/tick 275.9   sec/kimg 68.99   maintenance 0.2    cpumem 8.10   gpumem 4.85   augment 0.092\n",
            "tick 44    kimg 176.0    time 3h 55m 06s   sec/tick 277.0   sec/kimg 69.25   maintenance 0.1    cpumem 8.10   gpumem 4.87   augment 0.093\n",
            "tick 45    kimg 180.0    time 3h 59m 43s   sec/tick 276.5   sec/kimg 69.13   maintenance 0.2    cpumem 8.10   gpumem 4.86   augment 0.084\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 66.46732362062599}, \"metric\": \"fid50k_full\", \"total_time\": 599.2265863418579, \"total_time_str\": \"9m 59s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000180.pkl\", \"timestamp\": 1648472341.5455194}\n",
            "tick 46    kimg 184.0    time 4h 14m 38s   sec/tick 277.4   sec/kimg 69.35   maintenance 617.7  cpumem 8.10   gpumem 4.86   augment 0.086\n",
            "tick 47    kimg 188.0    time 4h 19m 15s   sec/tick 276.6   sec/kimg 69.15   maintenance 0.2    cpumem 8.10   gpumem 4.92   augment 0.098\n",
            "tick 48    kimg 192.0    time 4h 23m 52s   sec/tick 277.1   sec/kimg 69.26   maintenance 0.1    cpumem 8.10   gpumem 4.85   augment 0.097\n",
            "tick 49    kimg 196.0    time 4h 28m 29s   sec/tick 277.0   sec/kimg 69.24   maintenance 0.2    cpumem 8.10   gpumem 4.85   augment 0.095\n",
            "tick 50    kimg 200.0    time 4h 33m 07s   sec/tick 277.7   sec/kimg 69.42   maintenance 0.2    cpumem 8.10   gpumem 4.84   augment 0.091\n",
            "tick 51    kimg 204.0    time 4h 37m 44s   sec/tick 276.9   sec/kimg 69.23   maintenance 0.2    cpumem 8.10   gpumem 4.86   augment 0.086\n",
            "tick 52    kimg 208.0    time 4h 42m 22s   sec/tick 277.9   sec/kimg 69.47   maintenance 0.1    cpumem 8.10   gpumem 4.84   augment 0.086\n",
            "tick 53    kimg 212.0    time 4h 46m 59s   sec/tick 277.2   sec/kimg 69.30   maintenance 0.2    cpumem 8.10   gpumem 4.83   augment 0.081\n",
            "tick 54    kimg 216.0    time 4h 51m 37s   sec/tick 277.4   sec/kimg 69.35   maintenance 0.2    cpumem 8.10   gpumem 4.86   augment 0.083\n",
            "tick 55    kimg 220.0    time 4h 56m 14s   sec/tick 277.3   sec/kimg 69.34   maintenance 0.2    cpumem 8.10   gpumem 4.83   augment 0.081\n",
            "tick 56    kimg 224.0    time 5h 00m 52s   sec/tick 277.8   sec/kimg 69.44   maintenance 0.2    cpumem 8.10   gpumem 4.88   augment 0.088\n",
            "tick 57    kimg 228.0    time 5h 05m 29s   sec/tick 277.1   sec/kimg 69.27   maintenance 0.2    cpumem 8.10   gpumem 4.86   augment 0.083\n",
            "tick 58    kimg 232.0    time 5h 10m 07s   sec/tick 277.8   sec/kimg 69.45   maintenance 0.1    cpumem 8.10   gpumem 4.86   augment 0.081\n",
            "tick 59    kimg 236.0    time 5h 14m 45s   sec/tick 277.1   sec/kimg 69.27   maintenance 0.2    cpumem 8.10   gpumem 4.84   augment 0.087\n",
            "tick 60    kimg 240.0    time 5h 19m 22s   sec/tick 277.4   sec/kimg 69.34   maintenance 0.2    cpumem 8.10   gpumem 4.85   augment 0.085\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 60.464530409789276}, \"metric\": \"fid50k_full\", \"total_time\": 599.8971517086029, \"total_time_str\": \"10m 00s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000240.pkl\", \"timestamp\": 1648477121.6801748}\n",
            "tick 61    kimg 244.0    time 5h 34m 18s   sec/tick 277.8   sec/kimg 69.44   maintenance 618.4  cpumem 8.11   gpumem 4.83   augment 0.083\n",
            "tick 62    kimg 248.0    time 5h 38m 56s   sec/tick 277.6   sec/kimg 69.41   maintenance 0.2    cpumem 8.11   gpumem 4.83   augment 0.077\n",
            "tick 63    kimg 252.0    time 5h 43m 34s   sec/tick 277.3   sec/kimg 69.33   maintenance 0.2    cpumem 8.11   gpumem 4.86   augment 0.078\n",
            "tick 64    kimg 256.0    time 5h 48m 11s   sec/tick 277.5   sec/kimg 69.36   maintenance 0.2    cpumem 8.11   gpumem 4.83   augment 0.075\n",
            "tick 65    kimg 260.0    time 5h 52m 48s   sec/tick 276.9   sec/kimg 69.21   maintenance 0.2    cpumem 8.11   gpumem 4.86   augment 0.081\n",
            "tick 66    kimg 264.0    time 5h 57m 26s   sec/tick 277.4   sec/kimg 69.34   maintenance 0.2    cpumem 8.11   gpumem 4.83   augment 0.074\n",
            "tick 67    kimg 268.0    time 6h 02m 03s   sec/tick 276.8   sec/kimg 69.19   maintenance 0.2    cpumem 8.11   gpumem 4.82   augment 0.076\n",
            "tick 68    kimg 272.0    time 6h 06m 41s   sec/tick 278.0   sec/kimg 69.50   maintenance 0.2    cpumem 8.11   gpumem 4.89   augment 0.077\n",
            "tick 69    kimg 276.0    time 6h 11m 19s   sec/tick 277.6   sec/kimg 69.40   maintenance 0.2    cpumem 8.11   gpumem 4.86   augment 0.076\n",
            "tick 70    kimg 280.0    time 6h 15m 56s   sec/tick 277.2   sec/kimg 69.31   maintenance 0.2    cpumem 8.11   gpumem 4.86   augment 0.077\n",
            "tick 71    kimg 284.0    time 6h 20m 33s   sec/tick 277.0   sec/kimg 69.25   maintenance 0.2    cpumem 8.11   gpumem 4.84   augment 0.081\n",
            "tick 72    kimg 288.0    time 6h 25m 11s   sec/tick 277.3   sec/kimg 69.32   maintenance 0.2    cpumem 8.11   gpumem 4.84   augment 0.076\n",
            "tick 73    kimg 292.0    time 6h 29m 48s   sec/tick 277.1   sec/kimg 69.27   maintenance 0.2    cpumem 8.11   gpumem 4.86   augment 0.077\n",
            "tick 74    kimg 296.0    time 6h 34m 25s   sec/tick 277.3   sec/kimg 69.32   maintenance 0.2    cpumem 8.11   gpumem 4.84   augment 0.077\n",
            "tick 75    kimg 300.0    time 6h 39m 01s   sec/tick 275.8   sec/kimg 69.22   maintenance 0.1    cpumem 8.11   gpumem 4.86   augment 0.077\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 57.24389181442412}, \"metric\": \"fid50k_full\", \"total_time\": 598.7685875892639, \"total_time_str\": \"9m 59s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000300.pkl\", \"timestamp\": 1648481899.5788105}\n",
            "\n",
            "Exiting...\n"
          ]
        }
      ]
    }
  ]
}